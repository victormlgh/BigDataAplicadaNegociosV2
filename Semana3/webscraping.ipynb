{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Explorando la Reputación Corporativa: Parte de Web Scraping</h1>\n",
    "Por Víctor González"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Introducción</h1>\n",
    "En la era digital, la reputación corporativa se ha vuelto un recurso invaluable para las empresas, ejerciendo un poderoso influjo sobre su éxito, su capacidad para atraer talento y su vínculo con los consumidores. En este contexto, el análisis de sentimiento emerge como una herramienta poderosa para descifrar la percepción pública de una empresa en línea.\n",
    "\n",
    "El web scraping juega un papel crucial en la investigación de la reputación corporativa al permitir la recopilación automatizada de datos de diversas fuentes en línea, como redes sociales, sitios de noticias, foros y plataformas de revisión. Esta técnica facilita la obtención de una amplia gama de opiniones, comentarios y menciones sobre una empresa en particular, lo que proporciona una visión completa y actualizada de cómo es percibida por el público en general. Al analizar estos datos mediante técnicas como el análisis de sentimiento, es posible identificar tendencias, patrones y áreas de preocupación que pueden afectar la imagen y el rendimiento de la empresa. Además, el web scraping permite monitorear de manera continua y sistemática la reputación corporativa, lo que permite a las empresas detectar y abordar rápidamente cualquier problema o crisis que pueda surgir. \n",
    "\n",
    "En momentos críticos como los despidos masivos, la reputación corporativa puede sufrir un golpe significativo. La forma en que una empresa gestiona estas situaciones no solo influye en su imagen pública, sino también en su vínculo con empleados, clientes, inversores y otras partes interesadas clave. En este sentido, realizar un análisis de sentimiento en Twitter durante un evento de despidos masivos ofrece una valiosa perspectiva sobre la percepción de la empresa y las medidas que puede tomar para minimizar los daños potenciales.\n",
    "\n",
    "El propósito de este estudio es utilizar técnicas de web scraping para investigar la reputación corporativa de Facebook, ahora conocida como Meta, en el contexto de las opiniones expresadas en Twitter sobre los despidos masivos ocurridos en la empresa. Al recopilar y analizar datos de Twitter relacionados con este evento, buscamos comprender cómo la comunidad en línea percibe y reacciona ante los despidos masivos de Facebook, así como identificar tendencias, opiniones predominantes y áreas clave de preocupación y oportunidad para la gestión de la reputación corporativa de la empresa.\n",
    "\n",
    "El cuaderno <a href='https://github.com/victormlgh/BigDataAplicadaNegociosV2/blob/master/Semana3/sentimentanalisys.ipynb'>Explorando la Reputación Corporativa: Parte de Análisis de Sentimiento</a> se centra en el uso de los datos ya recopilados mediante web scraping para examinar cómo los comentarios y opiniones expresados en Twitter reflejan la reputación corporativa de la empresa en el contexto de los despidos masivos empleando diversas técnicas de análisis de sentimientos.\n",
    "\n",
    "<h1>Contexto</h1>\n",
    "\n",
    "\n",
    "En el año 2023, Meta, anteriormente conocida como Facebook, se enfrentó a una ola de despidos masivos que dejó una marcada huella en su reputación corporativa. Como líder en el ámbito de las redes sociales y la tecnología digital, la empresa atravesó un período tumultuoso que suscitó preocupaciones tanto internas como externas. Los despidos masivos no solo impactaron a los empleados directamente afectados, sino que también resonaron en la comunidad de usuarios, inversores y observadores de la industria tecnológica en general.\n",
    "\n",
    "Antes de los despidos masivos, Meta ya enfrentaba una creciente presión pública debido a inquietudes sobre la privacidad de datos, la regulación gubernamental y críticas acerca del impacto negativo de las redes sociales en la sociedad. A pesar de ello, la empresa mantenía una posición dominante en el mercado y una reputación sólida en términos de innovación tecnológica y crecimiento financiero.\n",
    "\n",
    "Durante el periodo de despidos masivos, Meta fue sometida a un escrutinio público intenso. Los recortes de personal, que impactaron a miles de empleados en diferentes áreas de la empresa, generaron controversias y críticas sobre las prácticas laborales y la cultura corporativa de Meta. Además, surgieron inquietudes sobre el impacto de los despidos en la moral de los empleados restantes y en la capacidad de la empresa para mantener su posición en un mercado altamente competitivo.\n",
    "\n",
    "Tras los despidos masivos, Meta emprendió una campaña para recuperar su imagen pública y reconstruir la confianza perdida. La empresa adoptó medidas para mejorar las condiciones laborales, comunicarse de manera más transparente con sus empleados y comprometerse con la comunidad en temas cruciales como la seguridad en línea y la responsabilidad corporativa. Sin embargo, el proceso de recuperación de la reputación corporativa resultó desafiante y continuo, ya que Meta enfrentó obstáculos persistentes para restablecer la confianza y la credibilidad.\n",
    "\n",
    "Para obtener una comprensión más profunda de cómo los despidos masivos impactaron la reputación de Meta, se realizó un análisis exhaustivo en la plataforma X, abarcando el período anterior, durante y después de los eventos. Este análisis ofrece una visión detallada de la evolución de la percepción pública de la empresa durante este período crítico, identificando tendencias, opiniones predominantes y áreas clave de preocupación y oportunidad para Meta en lo que respecta a la gestión de su reputación corporativa.\n",
    "\n",
    "\n",
    "<h1>Web Scraping: teoría e implementación </h1>\n",
    "\n",
    "La práctica del web scraping implica la extracción automatizada de datos de páginas web utilizando herramientas y técnicas especializadas. Teóricamente, el proceso consiste en enviar solicitudes HTTP a un sitio web, analizar la estructura del HTML recibido y extraer los datos relevantes según ciertos criterios predefinidos, como etiquetas HTML específicas, clases o identificadores.\n",
    "\n",
    "En la implementación práctica, se utilizan diversas bibliotecas y frameworks de programación, como Beautiful Soup en Python, para realizar estas tareas de forma eficiente. Estas herramientas permiten a los desarrolladores escribir scripts que navegan por el contenido web, identifican patrones y extraen la información deseada.\n",
    "\n",
    "Sin embargo, la implementación efectiva del web scraping no se limita solo a la extracción de datos, sino que también implica el manejo de desafíos como la gestión de la tasa de solicitud para evitar bloqueos por parte del servidor, el tratamiento de datos inconsistentes o incompletos, y el cumplimiento de las políticas de uso del sitio web objetivo.\n",
    "\n",
    "<h1>Código en Python</h1>\n",
    "\n",
    "Para trabajar en Python debemos importar las librerias que vamos a utilizar para cargar los datos y poder analizarlos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import math\n",
    "import random\n",
    "import sklearn\n",
    "from nltk.corpus import stopwords\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.sparse.linalg import svds\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "import re\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "import time\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Existen diversas formas de obtener datos de Twitter para su análisis. Una de las formas más recomendadas es utilizar el API (Interfaz de Programación de Aplicaciones) oficial de la plataforma, que proporciona acceso a una amplia gama de datos y funcionalidades. Sin embargo, es importante tener en cuenta que el API tiene una limitación importante: no permite recuperar tweets más allá de una ventana de una semana. Esta restricción significa que no es posible acceder a datos históricos más antiguos.\n",
    "\n",
    "Si el objetivo de la investigación es monitorear el uso de Twitter en tiempo real, el API sigue siendo la mejor opción, ya que permite obtener datos actualizados de manera instantánea. Sin embargo, en casos donde se necesita analizar un período específico en el pasado, como en nuestro estudio sobre los despidos masivos de Meta, el uso del API no es viable debido a su limitación en la recuperación de datos históricos.\n",
    "\n",
    "En estas circunstancias, se recurre a técnicas de web scraping para obtener los datos necesarios. El web scraping involucra la extracción automatizada de información de páginas web, lo que nos permite acceder a tweets antiguos y otros datos de Twitter que no están disponibles a través del API. Aunque el web scraping puede ser más complejo y requiere un mayor esfuerzo técnico, es una solución efectiva para recopilar datos históricos y llevar a cabo análisis retrospectivos en situaciones como la que nos concierne.\n",
    "\n",
    "<h1>Web Scraping</h1>\n",
    "\n",
    "Para realizar el web scraping, vamos a emplear una combinación de librerías que nos proporcionarán las herramientas necesarias para acceder y extraer datos de las páginas web de manera efectiva. En este caso, utilizaremos:\n",
    "\n",
    "- Selenium: Esta librería nos permite controlar un navegador web de forma automatizada. Podremos simular la interacción humana con la página web, lo que resulta útil para páginas que requieren acciones dinámicas como hacer clic en botones o completar formularios.\n",
    "\n",
    "- BeautifulSoup: Esta librería es una potente herramienta de análisis HTML y XML. Nos permite parsear el contenido HTML de una página web y extraer la información deseada de una manera sencilla y eficiente.\n",
    "\n",
    "- Requests: Esta librería nos facilita realizar peticiones HTTP a una página web y obtener su contenido. Es útil para obtener el código fuente de una página web que posteriormente podemos analizar con BeautifulSoup.\n",
    "\n",
    "La combinación de estas tres librerías nos brinda la flexibilidad y el control necesarios para realizar el web scraping de manera efectiva y eficiente, permitiéndonos recopilar los datos necesarios para nuestro análisis de reputación corporativa relacionado con los despidos masivos de Meta.\n",
    "\n",
    "<h2>Entender el URL</h2>\n",
    "\n",
    "El URL que vamos a utilizar para la busqueda se compone de varios elementos que especifican la ubicación y los parámetros de la búsqueda en Twitter:\n",
    "\n",
    "- Protocolo (https://): Indica el protocolo de transferencia de hipertexto seguro, que garantiza una conexión segura entre el navegador y el servidor web.\n",
    "- Dominio (twitter.com): Es el nombre de dominio de Twitter, donde se aloja el servicio web.\n",
    "- Ruta (/search): Es la ruta de la URL que especifica la ubicación del recurso que se solicita en el servidor. En este caso, \"/search\" indica que estamos accediendo a la función de búsqueda de Twitter.\n",
    "- Parámetros de la búsqueda (?q=facebook&src=typed_query): Estos son los parámetros que se utilizan para personalizar la búsqueda. En este caso:\n",
    "    - q=facebook: Es el parámetro de consulta que especifica el término de búsqueda, en este caso, \"facebook\". Esto indica que estamos buscando tweets que contengan la palabra \"facebook\".\n",
    "    - src=typed_query: Es un parámetro que indica la fuente de la consulta. En este caso, \"typed_query\" significa que la búsqueda se realizó a través de la barra de búsqueda y se escribió directamente por el usuario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<!DOCTYPE html>\\n<html dir=\"ltr\" lang=\"en\">\\n<head>\\n  <meta charset=\"utf-8\" />\\n  <meta name=\"viewport\"\\n    content=\"width=device-width,initial-scale=1,maximum-scale=1,user-scalable=0,viewport-fit=cover\" />\\n  <style>\\n    body {\\n      -ms-overflow-style: scrollbar;\\n      overflow-y: scroll;\\n      overscroll-behavior-y: none;\\n    }\\n\\n    .errorContainer {\\n      background-color: #FFF;\\n      color: #0F1419;\\n      max-width: 600px;\\n      margin: 0 auto;\\n      padding: 10%;\\n      font-family: Helvetica, sans-serif;\\n      font-size: 16px;\\n    }\\n\\n    .errorButton {\\n      margin: 3em 0;\\n    }\\n\\n    .errorButton a {\\n      background: #1DA1F2;\\n      border-radius: 2.5em;\\n      color: white;\\n      padding: 1em 2em;\\n      text-decoration: none;\\n    }\\n\\n    .errorButton a:hover,\\n    .errorButton a:focus {\\n      background: rgb(26, 145, 218);\\n    }\\n\\n    .errorFooter {\\n      color: #657786;\\n      font-size: 80%;\\n      line-height: 1.5;\\n      padding: 1em 0;\\n    }\\n\\n    .errorFooter a,\\n    .errorFooter a:visited {\\n      color: #657786;\\n      text-decoration: none;\\n      padding-right: 1em;\\n    }\\n\\n    .errorFooter a:hover,\\n    .errorFooter a:active {\\n      text-decoration: underline;\\n    }\\n\\n  </style>\\n</head>\\n<body>\\n  <div class=\"errorContainer\">\\n    <img width=\"46\" height=\"38\"\\n      srcset=\"https://abs.twimg.com/errors/logo46x38.png 1x, https://abs.twimg.com/errors/logo46x38@2x.png 2x\"\\n      src=\"https://abs.twimg.com/errors/logo46x38.png\" alt=\"Twitter\" />\\n    <h1>This browser is no longer supported.</h1>\\n    <p>\\n      Please switch to a supported browser to continue using twitter.com. You can see a list of supported browsers in our Help Center.\\n    </p>\\n    <p class=\"errorButton\"><a href=\"https://help.twitter.com/using-twitter/twitter-supported-browsers\">Help Center</a>\\n    </p>\\n    <p class=\"errorFooter\">\\n      <a href=\"https://twitter.com/tos\">Terms of Service</a>\\n      <a href=\"https://twitter.com/privacy\">Privacy Policy</a>\\n      <a href=\"https://support.twitter.com/articles/20170514\">Cookie Policy</a>\\n      <a href=\"https://legal.twitter.com/imprint.html\">Imprint</a>\\n      <a href=\"https://business.twitter.com/en/help/troubleshooting/how-twitter-ads-work.html?ref=web-twc-ao-gbl-adsinfo&utm_source=twc&utm_medium=web&utm_campaign=ao&utm_content=adsinfo\">Ads info</a>\\n      © 2024 X Corp.\\n    </p>\\n\\n  </div>\\n</body>\\n</html>'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "url = 'https://twitter.com/search?q=facebook&src=typed_query'\n",
    "response = requests.get(url)\n",
    "display(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al visitar esta URL, el navegador realiza una solicitud al servidor de Twitter, que responde enviando el código HTML de la página de resultados de búsqueda correspondiente. Este código HTML contiene toda la estructura y el contenido de la página web, incluyendo los tweets, los perfiles de usuario, los botones de interacción y cualquier otro elemento visible en la página.\n",
    "\n",
    "Al extraer el código HTML de esta página web, podemos analizar su estructura y contenido utilizando herramientas como BeautifulSoup en Python. Esto nos permite acceder a la información de los tweets, como su contenido, fecha de publicación, número de retweets y favoritos, entre otros datos. Además, podemos realizar operaciones de filtrado, clasificación y análisis sobre estos datos para obtener información relevante y útil para nuestro propósito de estudio, como la percepción pública sobre Facebook en Twitter en relación con los despidos masivos.\n",
    "\n",
    "Twitter emplea una página web dinámica para presentar su contenido, lo que significa que gran parte de lo que vemos en la interfaz se genera en tiempo real en el navegador del usuario mediante JavaScript y otras tecnologías similares. Esta característica implica varios desafíos significativos para el proceso de web scraping.\n",
    "\n",
    "En primer lugar, debido a la carga dinámica de contenido, gran parte de la información no está presente en el HTML inicial de la página, sino que se agrega posteriormente a medida que el usuario interactúa con la plataforma. Por ejemplo, los tweets y las respuestas pueden cargarse a medida que se desplaza hacia abajo en el feed, lo que dificulta la extracción de estos datos sin la capacidad de interactuar con la página como lo haría un usuario humano.\n",
    "\n",
    "Además, Twitter utiliza interacciones complejas del usuario, como el desplazamiento infinito, que permite cargar más contenido a medida que el usuario se desplaza hacia abajo en la página. Estas interacciones, controladas por JavaScript, pueden ser difíciles de replicar de manera automatizada con técnicas de web scraping convencionales.\n",
    "\n",
    "Selenium es una herramienta muy útil cuando se trata de realizar web scraping en páginas web dinámicas como Twitter. Una de las principales ventajas de Selenium es su capacidad para automatizar la interacción con el navegador web, lo que nos permite simular el comportamiento humano al navegar por la página y realizar acciones como hacer clic en botones, desplazarnos por el contenido y completar formularios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'profile_name': 'Facebook',\n",
       " 'profile_handle': '@facebook',\n",
       " 'profile_bio': 'We believe people can do more together than alone and that each of us plays an important role in helping to create this safe and respectful community.',\n",
       " 'profile_category': None,\n",
       " 'profile_website': 'https://t.co/Xbu7FLA92S',\n",
       " 'profile_joining_date': 'Joined September 2019',\n",
       " 'profile_following': '16 Following',\n",
       " 'profile_followers': '587.6K Followers'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def scrapper(url):\n",
    "    data={}\n",
    "    options = Options()\n",
    "    options.add_argument('--headless=new')\n",
    "\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(url)\n",
    "    time.sleep(2)\n",
    "    resp = driver.page_source\n",
    "    driver.close()\n",
    "    soup=BeautifulSoup(resp,'html.parser')\n",
    "\n",
    "    try:\n",
    "        data[\"profile_name\"]=soup.find(\"div\",{\"class\":\"r-1vr29t4\"}).text\n",
    "    except:\n",
    "        data[\"profile_name\"]=None\n",
    "\n",
    "    try:\n",
    "        data[\"profile_handle\"]=soup.find(\"div\",{\"class\":\"r-1wvb978\"}).text\n",
    "    except:\n",
    "        data[\"profile_handle\"]=None\n",
    "\n",
    "    try:\n",
    "        data[\"profile_bio\"]=soup.find(\"div\",{\"data-testid\":\"UserDescription\"}).text\n",
    "    except:\n",
    "        data[\"profile_bio\"]=None\n",
    "\n",
    "    profile_header = soup.find(\"div\",{\"data-testid\":\"UserProfileHeader_Items\"})\n",
    "\n",
    "    try:\n",
    "        data[\"profile_category\"]=profile_header.find(\"span\",{\"data-testid\":\"UserProfessionalCategory\"}).text\n",
    "    except:\n",
    "        data[\"profile_category\"]=None\n",
    "\n",
    "    try:\n",
    "        data[\"profile_website\"]=profile_header.find('a').get('href')\n",
    "    except:\n",
    "        data[\"profile_website\"]=None\n",
    "\n",
    "    try:\n",
    "        data[\"profile_joining_date\"]=profile_header.find(\"span\",{\"data-testid\":\"UserJoinDate\"}).text\n",
    "    except:\n",
    "        data[\"profile_joining_date\"]=None\n",
    "\n",
    "    try:\n",
    "        data[\"profile_following\"]=soup.find_all(\"a\",{\"class\":\"r-rjixqe\"})[0].text\n",
    "    except:\n",
    "        data[\"profile_following\"]=None\n",
    "\n",
    "    try:\n",
    "        data[\"profile_followers\"]=soup.find_all(\"a\",{\"class\":\"r-rjixqe\"})[1].text\n",
    "    except:\n",
    "        data[\"profile_followers\"]=None\n",
    "\n",
    "    return data\n",
    "\n",
    "target_url = \"https://twitter.com/facebook\"\n",
    "\n",
    "display(scrapper(target_url))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La función scrapper toma una URL de cualquier usuario de Twitter y, mediante técnicas de web scraping, obtiene información del perfil público de ese usuario. Esto implica acceder a la página del perfil de Twitter correspondiente a la URL proporcionada y extraer datos relevantes, como el nombre de usuario, la biografía, la ubicación, el número de seguidores, el número de tweets, la fecha de creación de la cuenta, entre otros detalles disponibles públicamente en el perfil.\n",
    "\n",
    "Para realizar esta tarea, la función puede utilizar librerías de Python como Selenium, BeautifulSoup y Requests para enviar solicitudes HTTP, analizar el código HTML de la página del perfil y extraer la información deseada. Esto permite obtener una representación estructurada de los datos del perfil de Twitter, que luego pueden ser utilizados para diversos propósitos, como análisis de usuarios, seguimiento de cambios en los perfiles, o cualquier otra aplicación que requiera acceder a información pública de los usuarios de Twitter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'profile_name': None,\n",
       " 'profile_handle': None,\n",
       " 'profile_bio': None,\n",
       " 'profile_category': None,\n",
       " 'profile_website': None,\n",
       " 'profile_joining_date': None,\n",
       " 'profile_following': None,\n",
       " 'profile_followers': None}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(scrapper(url))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso, es importante tener en cuenta que Twitter impide el acceso a ciertas funciones, como la búsqueda, sin estar autenticado en el sistema. Esto significa que, para realizar la búsqueda de tweets específicos a través de la URL proporcionada, se requeriría una sesión iniciada en Twitter.\n",
    "\n",
    "Dado que el acceso a la búsqueda en Twitter no es posible sin estar autenticado, no podemos realizar directamente el scraping de la página de resultados de búsqueda a través de la URL proporcionada. En este escenario, sería necesario utilizar otra estrategia, como autenticarse en Twitter utilizando Selenium para poder acceder a la función de búsqueda y luego realizar el scraping de los resultados obtenidos.\n",
    "\n",
    "<h2>Ejemplo de un Scrapper</h2>\n",
    "\n",
    "Vamos a usar un pequeño ejemplo para ilustrar cómo podemos realizar web scraping para crear un conjunto de datos. En este ejemplo, vamos a obtener el nombre y el precio de los productos del catálogo de una página de prueba.\n",
    "\n",
    "Primero, utilizaremos técnicas de web scraping para acceder a la página web del catálogo de productos. Luego, analizaremos el código HTML de la página para identificar los elementos que contienen la información que deseamos extraer, como el nombre y el precio de cada producto.\n",
    "\n",
    "Una vez identificados estos elementos, utilizaremos una combinación de herramientas como BeautifulSoup y requests en Python para extraer el nombre y el precio de cada producto de la página web. Estos datos se almacenarán en un diccionario para crear nuestro conjunto de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "driver.get('https://sandbox.oxylabs.io/products')\n",
    "content = driver.page_source\n",
    "soup = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con esta función, extraeremos los nombres de los productos del catálogo de la página web. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for a in soup.find_all(attrs={'class': 'product-card'}):\n",
    "    name = a.find('h4')\n",
    "    if name not in results:\n",
    "        results.append(name.text)\n",
    "\n",
    "df['name'] = pd.DataFrame({'name':results})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego, obtendremos los precios de dichos productos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for element in soup.findAll(attrs={'class': 'product-card'}):\n",
    "    name = element.find(attrs={'class': 'price-wrapper'})\n",
    "    if name not in results:\n",
    "        results.append(name.text)\n",
    "\n",
    "df['cost'] = pd.DataFrame({'cost':results})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Después de extraer los nombres de los productos y sus precios, podemos mostrar una muestra de los datos y sus dimensiones para tener una idea de cómo se ve nuestro conjunto de datos y cuántos elementos contiene. Esto nos permitirá verificar que la extracción de datos se realizó correctamente y que el conjunto de datos tiene el formato y la estructura esperados antes de realizar cualquier análisis adicional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 2)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>cost</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The Legend of Zelda: Ocarina of Time</td>\n",
       "      <td>91,99 €</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Super Mario Galaxy</td>\n",
       "      <td>91,99 €</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Super Mario Galaxy 2</td>\n",
       "      <td>91,99 €</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Metroid Prime</td>\n",
       "      <td>89,99 €</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Super Mario Odyssey</td>\n",
       "      <td>89,99 €</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   name     cost\n",
       "0  The Legend of Zelda: Ocarina of Time  91,99 €\n",
       "1                    Super Mario Galaxy  91,99 €\n",
       "2                  Super Mario Galaxy 2  91,99 €\n",
       "3                         Metroid Prime  89,99 €\n",
       "4                   Super Mario Odyssey  89,99 €"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df.shape)\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Retornando a los datos de Twitter/X</h1>\n",
    "\n",
    "Con una lista de usuarios de la plataforma, las posibilidades de crear un conjunto de datos extraído por web scraping son amplias y variadas. Aprovechando esta lista, podemos automatizar el proceso de recopilación de datos de múltiples perfiles de usuario en la plataforma. Esto nos permite obtener una gran cantidad de información sobre cada usuario, incluyendo detalles como su nombre, alias, ubicación, biografía, número de seguidores, número de tweets publicados, interacciones, entre otros.\n",
    "\n",
    "Al utilizar técnicas de web scraping de manera eficiente, podemos extraer estos datos de manera sistemática y estructurada, almacenándolos en un formato que sea fácil de analizar y procesar. Por ejemplo, podríamos crear un conjunto de datos en formato CSV, donde cada fila representa un usuario y cada columna contiene diferentes atributos o características del usuario.\n",
    "\n",
    "Además de los datos básicos del perfil, también podríamos profundizar en la actividad del usuario, como sus tweets más recientes, sus interacciones con otros usuarios, la frecuencia y el contenido de sus publicaciones, entre otros aspectos. Esto nos proporcionaría una visión más completa y detallada de cada usuario y de su actividad en la plataforma.\n",
    "\n",
    "Este conjunto de datos resultante puede ser utilizado para una variedad de propósitos, como análisis de la comunidad de usuarios, identificación de tendencias, segmentación de usuarios, detección de bots o cuentas falsas, entre otros. En resumen, con una lista de usuarios de la plataforma y el uso adecuado de técnicas de web scraping, podemos obtener una valiosa fuente de datos que nos permitirá entender mejor el comportamiento y las características de la comunidad de usuarios en la plataforma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>profile</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://twitter.com/elonmusk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://twitter.com/BarackObama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://twitter.com/Cristiano</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://twitter.com/justinbieber</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://twitter.com/rihanna</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            profile\n",
       "0      https://twitter.com/elonmusk\n",
       "1   https://twitter.com/BarackObama\n",
       "2     https://twitter.com/Cristiano\n",
       "3  https://twitter.com/justinbieber\n",
       "4       https://twitter.com/rihanna"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = pd.read_csv('name.csv')\n",
    "names.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con una lista de usuarios de la plataforma, podemos observar los perfiles individuales de cada usuario para obtener información detallada sobre ellos. Utilizando técnicas de web scraping, podemos automatizar este proceso y recopilar datos específicos de cada perfil, como el nombre, la biografía, la ubicación, el número de seguidores, el número de tweets publicados y otras métricas relevantes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for i, row in names.iterrows():\n",
    "    data = scrapper(row['profile'])\n",
    "    results.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 8)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame.from_records(results)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con estas funciones, podemos crear nuestro propio conjunto de datos de perfiles de Twitter utilizando web scraping. Al recopilar información detallada de cada perfil, como nombres, biografías, ubicaciones, número de seguidores y otras métricas relevantes, creamos un conjunto de datos completo que nos proporciona una visión general de la comunidad de usuarios en la plataforma.\n",
    "\n",
    "Este dataset nos permite realizar análisis profundos sobre los usuarios de Twitter, como identificar tendencias, comprender comportamientos y preferencias, segmentar la audiencia y mucho más. Además, al automatizar el proceso de recopilación de datos mediante web scraping, podemos actualizar fácilmente nuestro conjunto de datos para mantenerlo relevante y actualizado en todo momento.\n",
    "\n",
    "<h1>Conclusión</h1>\n",
    "\n",
    "El uso de web scraping para obtener datos de Twitter con el fin de medir la reputación de una empresa puede ser una herramienta poderosa y valiosa. Sin embargo, es importante tener en cuenta que Twitter ha implementado mecanismos para limitar las respuestas de su plataforma ante eventos de web scraping. Estas medidas pueden ralentizar las implementaciones y dificultar la obtención de datos de manera eficiente y rápida.\n",
    "\n",
    "A pesar de estos desafíos, el web scraping sigue siendo una técnica viable para recopilar datos relevantes de Twitter y analizar la percepción pública de una empresa. Al utilizar enfoques adecuados y herramientas especializadas, como Selenium, BeautifulSoup y otras librerías, es posible superar estas limitaciones y obtener información valiosa sobre cómo una empresa es percibida en la plataforma.\n",
    "\n",
    "Es importante tener en cuenta las políticas y términos de servicio de Twitter al realizar web scraping, y respetar los límites establecidos por la plataforma. Además, es fundamental realizar análisis éticos y precisos de los datos recopilados para evitar interpretaciones erróneas o sesgadas.\n",
    "\n",
    "En conclusión, a pesar de las medidas implementadas por Twitter para restringir el web scraping en su plataforma, esta técnica sigue siendo una herramienta valiosa para evaluar la reputación corporativa y obtener información crucial sobre la percepción pública de una empresa en línea. Con enfoques éticos y meticulosos, el web scraping puede generar valiosos insights que nutren las estrategias de comunicación, gestión de crisis y toma de decisiones empresariales.\n",
    "\n",
    "En el próximo cuaderno <a href='https://github.com/victormlgh/BigDataAplicadaNegociosV2/blob/master/Semana3/sentimentanalisys.ipynb'>Explorando la Reputación Corporativa: Parte de Análisis de Sentimiento</a>, emplearemos datos de Twitter para entrenar modelos de análisis de sentimiento y evaluar la reputación en línea de una empresa. Utilizaremos técnicas avanzadas de procesamiento de lenguaje natural para extraer información significativa de los tweets y desarrollar modelos que nos ayuden a comprender cómo el público percibe a la empresa en diferentes contextos. Esto nos permitirá no solo cuantificar la opinión pública, sino también identificar tendencias y patrones que puedan influir en la reputación corporativa. Con estos conocimientos, estaremos mejor equipados para tomar decisiones informadas y estratégicas en cuanto a la comunicación, gestión de crisis y desarrollo empresarial."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
