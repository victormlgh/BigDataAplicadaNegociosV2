{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Tarea</h1>\n",
    "<h3>Explorando la Reputación Corporativa</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Contexto</h1>\n",
    "\n",
    "En la era digital, la reputación corporativa se ha convertido en un activo crucial para las empresas, influyendo en su éxito, su capacidad para atraer talento y su relación con los consumidores. En este contexto, el análisis de sentimiento emerge como una herramienta poderosa para comprender la percepción pública de una empresa en línea.\n",
    "\n",
    "El análisis de sentimiento es una técnica que permite identificar, cuantificar y analizar las emociones y opiniones expresadas en texto, ya sea en redes sociales, reseñas de productos, artículos de noticias o cualquier otro tipo de contenido en línea. Al aplicar esta técnica al estudio de la reputación corporativa, se puede obtener una visión profunda de cómo una empresa es percibida por el público en general y cómo estas percepciones pueden influir en su imagen y desempeño.\n",
    "\n",
    "En esta serie de artículos, nos adentraremos en el emocionante campo del análisis de sentimiento aplicado a la reputación corporativa. Exploraremos cómo esta técnica puede ayudar a las empresas a monitorear la opinión pública, identificar tendencias emergentes, evaluar la satisfacción del cliente y detectar posibles problemas de reputación.\n",
    "\n",
    "<h1>Asignación</h1>\n",
    "\n",
    "Después de revisar los cuaderno sobre \"Explorando la Reputación Corporativa: Parte de Web Scraping\" y \"Explorando la Reputación Corporativa: Parte de Análisis de Sentimiento\", tenemos el objetivo de desarrollar un sistema de análisis de sentimiento basado en Big Data para explorar la reputación corporativa de Facebook, con el fin de comprender la percepción del público y detectar tendencias y patrones relevantes.\n",
    "\n",
    "Como experto en Big Data, se le solicita desarrollar este sistema de análisis de sentimiento utilizando técnicas avanzadas de procesamiento de lenguaje natural y aprendizaje automático. El sistema deberá recopilar y analizar datos de diversas fuentes en línea, como redes sociales, foros, noticias y reseñas de productos, para identificar la opinión y el sentimiento del público hacia la Empresa.\n",
    "\n",
    "<h1>Tareas</h1>\n",
    "\n",
    "- Recopilación de datos: Obtener datos en línea de diversas fuentes relevantes para la reputación corporativa de [Nombre de la Empresa], como Twitter, Facebook, blogs, noticias, etc.\n",
    "- Preprocesamiento de datos: Limpiar y preparar los datos para su análisis, incluyendo la eliminación de ruido, la normalización de texto y la tokenización.\n",
    "- Desarrollo de algoritmos de análisis de sentimiento: Diseñar y desarrollar algoritmos de análisis de sentimiento utilizando técnicas de aprendizaje automático, como modelos de clasificación de texto y análisis de emociones.\n",
    "- Implementación del sistema: Implementar el sistema de análisis de sentimiento en una infraestructura de Big Data escalable y eficiente, asegurando su capacidad para procesar grandes volúmenes de datos en tiempo real.\n",
    "- Evaluación del sistema: Evaluar el rendimiento del sistema utilizando métricas adecuadas, como precisión, recall y F1-score, y realizar ajustes según sea necesario.\n",
    "- Generación de informes: Generar informes periódicos que resuman los resultados del análisis de sentimiento y destaquen las tendencias y patrones identificados.\n",
    "\n",
    "<h1>Código en Python</h1>\n",
    "\n",
    "En esta sección, es fundamental cargar las diferentes bibliotecas que se utilizarán en el estudio para garantizar un análisis efectivo y eficiente de los datos. A continuación, se proporciona un ejemplo de cómo podrías cargar estas bibliotecas en Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "import re\n",
    "import contractions\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from wordcloud import WordCloud\n",
    "from textblob import TextBlob\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "import itertools\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los datos esta disponible para descargar directamente desde el repositorio de <a href='https://drive.google.com/file/d/1TwSiWvg-WAYfpgzsC3L-Dg3BEEIJkD-N/view?usp=drive_link'>datos</a>.\n",
    "\n",
    "El primer paso consiste en cargar los datos que utilizaremos para entrenar nuestros modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOMSEED=56\n",
    "\n",
    "df = pd.read_csv('training.csv',lineterminator='\\n')\n",
    "display(df.shape)\n",
    "display(df.head())\n",
    "display(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta etapa inicial, nos enfocamos en cargar específicamente la parte de los tweets de nuestros datos. Descartamos otras columnas o atributos que no son relevantes para nuestro análisis o modelo, centrándonos únicamente en la información contenida en los tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['Tweet']]\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Proceso de Limpieza</h1>\n",
    "Después de cargar los datos de los tweets, procedemos a realizar una limpieza exhaustiva de los mensajes mediante el desarrollo de una función especializada para este propósito. El proceso comienza convirtiendo todos los mensajes a formato de cadena de texto y luego los transformamos a minúsculas para mantener una coherencia en el análisis. Posteriormente, eliminamos todas las URLs, las menciones de usuarios (@user), los hashtags (#), emojis, números y signos de puntuación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cleaning(tweet):\n",
    "  tweet = str(tweet)\n",
    "  tweet = tweet.lower()\n",
    "  tweet = re.sub('http\\S+|www\\S+|https\\S+', '', tweet, flags=re.MULTILINE)\n",
    "  tweet = re.sub('\\@\\w+|\\#', '', tweet)\n",
    "\n",
    "  regrex_pattern = re.compile(pattern = \"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"\n",
    "        u\"\\U0001F300-\\U0001F5FF\"\n",
    "        u\"\\U0001F680-\\U0001F6FF\"\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"\n",
    "        u\"\\U00002702-\\U000027B0\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags = re.UNICODE)\n",
    "  regrex_pattern.sub('',tweet)\n",
    "\n",
    "  tweet = ''.join(c for c in tweet if not c.isdigit())\n",
    "\n",
    "  expanded = []\n",
    "  for word in tweet.split():\n",
    "    expanded.append(contractions.fix(word))\n",
    "  tweet =  ' '.join(expanded)\n",
    "\n",
    "  tweet = re.sub('[^\\w\\s]', '', tweet)\n",
    "\n",
    "  stop_words = set(stopwords.words('english'))\n",
    "  tweet_tokens = word_tokenize(tweet)\n",
    "  filtered_texts = [word for word in tweet_tokens if word not in stop_words]\n",
    "\n",
    "  lemma = WordNetLemmatizer()\n",
    "  lemma_texts = (lemma.lemmatize(text, pos='a') for text in filtered_texts)\n",
    "\n",
    "  return \" \".join(lemma_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Tweet = df['Tweet'].apply(data_cleaning)\n",
    "display(df.Tweet.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df[df.duplicated()].shape)\n",
    "display(df[df.duplicated()].head())\n",
    "\n",
    "df = df.drop_duplicates('Tweet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nos interesa ver el conjunto de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Polaridad</h2>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polarity(Tweet):\n",
    "  return TextBlob(Tweet).sentiment.polarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nos interesa que se aplique la funcion de polaridad al conjunto de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['Polarity'] = df['Tweet'].apply()\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Categorias</h2>\n",
    "Nos interesa que se aplique la funcion de Categorias al conjunto de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(score):\n",
    "  if score < 0:\n",
    "    return 'Negative'\n",
    "  elif score == 0:\n",
    "    return 'Neutral'\n",
    "  else:\n",
    "    return 'Positive'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['Sentiment'] = df['Polarity'].apply()\n",
    "#df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Análisis exploratorio de datos</h1>\n",
    "Presenta un wordcloud del conjunto de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ' '.join([tweets for tweets in df['Tweet']])\n",
    "plt.figure(figsize = (20,15))\n",
    "wordCloud = WordCloud(width = 1600, height = 800, random_state = 21).generate(words)\n",
    "plt.imshow(wordCloud, interpolation = \"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Divide tu data para entrenar el modelo y poder hacer pruebas de los modelos</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(df[\"Tweet\"], df[\"Sentiment\"], test_size=0.2, random_state=RANDOMSEED)\n",
    "display(X_train.shape)\n",
    "display(X_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Extracción de Características</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vect = TfidfVectorizer(sublinear_tf=True).fit(X_train)\n",
    "\n",
    "feature_names = vect.get_feature_names_out()\n",
    "display(\"Numero de features: {}\".format(len(feature_names)))\n",
    "X_train = vect.transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Entrenamiento de los modelos </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(predictions, train_predictions):\n",
    "    \"\"\"Comparar el rendimiento del modelo de aprendizaje automático con la referencia. Calculo estadísticos y Muestra de la curva ROC.\"\"\"\n",
    "    \n",
    "    results = {}\n",
    "    results['acurracy'] = accuracy_score(predictions, y_val)\n",
    "\n",
    "    train_results = {}\n",
    "    train_results['acurracy'] = accuracy_score(train_predictions, y_train)\n",
    "\n",
    "    for metric in ['acurracy']:\n",
    "        display(f'Comparación {metric.capitalize()} - Prueba: {round(results[metric], 2)} Entrenamiento: {round(train_results[metric], 2)}')\n",
    "\n",
    "    display('Reporte de Prueba')\n",
    "    print(classification_report(y_val, predictions))\n",
    "    display('Reporte de Entrenamiento')\n",
    "    print(classification_report(y_train, train_predictions))\n",
    "\n",
    "    return accuracy_score(predictions, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(predictions):\n",
    "\n",
    "    cm = confusion_matrix(y_val, predictions)\n",
    "    classes = ['Negativo', 'Neutro', 'Positivo']\n",
    "\n",
    "    plt.figure(figsize = (10, 10))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Oranges)\n",
    "    plt.title('Matriz de confusión', size = 24)\n",
    "    plt.colorbar(aspect=4)\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45, size = 14)\n",
    "    plt.yticks(tick_marks, classes, size = 14)\n",
    "\n",
    "    fmt = '.2f'\n",
    "    thresh = cm.max() / 2.\n",
    "\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt), fontsize = 20,\n",
    "                    horizontalalignment=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "        \n",
    "    plt.grid(None)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('Etiqueta Verdadera')\n",
    "    plt.xlabel('Etiqueta predicha')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Linear Support Vector Classifier (SVM)</h2>\n",
    "\n",
    "Vamos a definir el primer modelo a entrenar. \n",
    "\n",
    "Nos gustaría presentar la evaluación del modelo\n",
    "\n",
    "¿Podrías explicar lo que entiendes del resulato?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_svm = LinearSVC()\n",
    "model_svm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions_svm = model_svm.predict(X_train)\n",
    "predictions_svm = model_svm.predict(vect.transform(X_val))\n",
    "\n",
    "acurracy_svm = evaluate_model())\n",
    "plot_confusion_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(predictions_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Regresión Logística</h2>\n",
    "\n",
    "Vamos a definir el segundo modelo a entrenar. \n",
    "\n",
    "Nos gustaría presentar la evaluación del modelo\n",
    "\n",
    "¿Podrías explicar lo que entiendes del resulato?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lr = LogisticRegression()\n",
    "model_lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions_lr = model_lr.predict(X_train)\n",
    "predictions_lr = model_lr.predict(vect.transform(X_val))\n",
    "\n",
    "acurracy_lr = evaluate_model()\n",
    "plot_confusion_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Multinomial Naive Bayes</h2>\n",
    "\n",
    "Vamos a definir el tercer modelo a entrenar. \n",
    "\n",
    "Nos gustaría presentar la evaluación del modelo\n",
    "\n",
    "¿Podrías explicar lo que entiendes del resulato?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_nb = MultinomialNB()\n",
    "model_nb.fit(X_train, y_train)\n",
    "\n",
    "train_predictions_nb = model_nb.predict(X_train)\n",
    "predictions_nb = model_nb.predict(vect.transform(X_val))\n",
    "\n",
    "acurracy_nb = evaluate_model()\n",
    "plot_confusion_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Resultado</h1>\n",
    "\n",
    "Muestra un resumen de los resultados de todos los modelos\n",
    "\n",
    "¿Podrías explicar lo que entiendes del resulato?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['LinearSVC (SVM)', 'Regresión logística', 'Multinomial Naive Bayes']\n",
    "values = np.array([acurracy_svm, acurracy_lr, acurracy_nb])\n",
    "sns.set_theme(style='whitegrid', rc={'figure.figsize':(6, 4)})  #  elige la apariencia\n",
    "sns.barplot(x=models, y=values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Pruebas</h2>\n",
    "\n",
    "Realiza una pruebas del sistema de recomendación simulando ser el usuario -8845298781299428018 para evaluar las recomendaciones de cada modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "before_df = pd.read_csv('before.csv')\n",
    "layoff_df = pd.read_csv('layoff.csv')\n",
    "after_df = pd.read_csv('after.csv')\n",
    "display(before_df.head())\n",
    "display(layoff_df.head())\n",
    "display(after_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "before_df = before_df[['Tweet']]\n",
    "before_df.Tweet = before_df['Tweet'].apply(data_cleaning)\n",
    "before_df = before_df.drop_duplicates('Tweet')\n",
    "display(before_df.shape)\n",
    "display(before_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "layoff_df = layoff_df[['Tweet']]\n",
    "layoff_df.Tweet = layoff_df['Tweet'].apply(data_cleaning)\n",
    "layoff_df = layoff_df.drop_duplicates('Tweet')\n",
    "display(layoff_df.shape)\n",
    "display(layoff_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "after_df = after_df[['Tweet']]\n",
    "after_df.Tweet = after_df['Tweet'].apply(data_cleaning)\n",
    "after_df = after_df.drop_duplicates('Tweet')\n",
    "display(after_df.shape)\n",
    "display(after_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "before_df['Sentiment'] = model_svm.predict(vect.transform(before_df['Tweet']))\n",
    "display(before_df.shape)\n",
    "display(before_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style='whitegrid', rc={'figure.figsize':(6, 4)})\n",
    "sns.countplot(data=before_df, x=\"Sentiment\", hue='Sentiment', palette='bone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layoff_df['Sentiment'] = model_svm.predict(vect.transform(layoff_df['Tweet']))\n",
    "display(layoff_df.shape)\n",
    "display(layoff_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style='whitegrid', rc={'figure.figsize':(6, 4)})\n",
    "sns.countplot(data=layoff_df, x=\"Sentiment\", hue='Sentiment', palette='bone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "after_df['Sentiment'] = model_svm.predict(vect.transform(after_df['Tweet']))\n",
    "display(after_df.shape)\n",
    "display(after_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style='whitegrid', rc={'figure.figsize':(6, 4)})\n",
    "sns.countplot(data=after_df, x=\"Sentiment\", hue='Sentiment', palette='bone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_cr(df):\n",
    "  pos = df[df.Sentiment == 'Positive']\n",
    "  pos_count = len(pos.index)\n",
    "  neg = df[df.Sentiment == 'Negative']\n",
    "  neg_count = len(neg.index)\n",
    "  cr = ((pos_count-neg_count)/(pos_count+neg_count))*100\n",
    "  return cr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cr_before = determine_cr(before_df)\n",
    "cr_layoff = determine_cr(layoff_df)\n",
    "cr_after = determine_cr(after_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = ['Antes', 'Durante', 'Despues']\n",
    "values = np.array([cr_before, cr_layoff, cr_after])\n",
    "sns.set_theme(style='whitegrid', rc={'figure.figsize':(6, 4)})  #  elige la apariencia\n",
    "sns.barplot(x=col, y=values)\n",
    "plt.ylabel(\"Reputación neta de la marca\")\n",
    "plt.title(\"Comparaciones netas de reputación de marca\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,3,figsize=(10,5))\n",
    "fig.tight_layout(pad=4.0)\n",
    "ax[0].title.set_text('Antes')\n",
    "ax[1].title.set_text('Durante')\n",
    "ax[2].title.set_text('Despues')\n",
    "sns.countplot(x = 'Sentiment', hue = 'Sentiment', palette='bone', data = before_df, ax=ax[0])\n",
    "sns.countplot(x = 'Sentiment', hue = 'Sentiment', palette='bone', data = layoff_df, ax=ax[1])\n",
    "sns.countplot(x = 'Sentiment', hue = 'Sentiment', palette='bone', data = after_df, ax=ax[2])\n",
    "plt.ylabel(\"Total\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Discusión y Conclusión</h1>\n",
    "\n",
    "Presenta tus conclusiones sobre el trabajo llevado a cabo"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
